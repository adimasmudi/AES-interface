{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used : cpu\n",
      "ini d {'answer_text': ['5 Dampak Negatif Perkembangan Teknologi:\\nKetergantungan Berlebihan: Orang-orang menjadi terlalu bergantung pada teknologi dan kehilangan kemampuan untuk menyelesaikan masalah sendiri.\\nKecanduan Gadget: Penggunaan gadget yang berlebihan dapat menyebabkan kecanduan dan mengganggu kesehatan mental dan fisik.\\nKesenjangan Digital: Tidak semua orang memiliki akses yang sama terhadap teknologi, sehingga terjadi kesenjangan antara yang kaya dan yang miskin.\\nKejahatan Siber: Teknologi dapat digunakan untuk melakukan kejahatan, seperti penipuan, pencurian data, dan penyebaran berita bohong.\\nPenyalahgunaan Informasi: Informasi yang beredar di internet tidak selalu akurat dan dapat disalahgunakan untuk menyebarkan kebencian dan propaganda.'], 'answer_input_ids': tensor([[    2,   418,  3345,  3778,  1657,  1429, 30472, 10971,  4799, 30472,\n",
      "           232, 30469,   232,   234,  1386,  6990,   126,  1429,    41,  3337,\n",
      "          1352,    90,  3426,   805,   536, 30470, 14671,  6734, 30472,  1852,\n",
      "          6734,    34,  4799,   173,  1618, 14671,    41,  4826,   964,  4784,\n",
      "            41,  2344, 30470, 15856,  4017, 30472,   119,   366,   232,   343,\n",
      "          2268,    34,   500,   618,  1429, 30468,   485,   597, 15856,   644,\n",
      "            34,  2913,    41,    34,  4846, 30470,  5236, 24819, 30472,  1429,\n",
      "           173,   781,    90,   464,  5236, 30468,   295,  7280, 30468, 11994,\n",
      "          1006, 30468,    41,  8359,  2140, 13169, 30470, 12922,   683, 30472,\n",
      "           683,    34,  5823,    26,  1454,   119,   811,  5485,    41,   173,\n",
      "         28397,    90,  9113, 14745,    41, 21727, 30470,     3,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'answer_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'student_text': ['gak tau saya bu'], 'student_input_ids': tensor([[   2, 1489, 2088,  209,  379,    3,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'student_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "pooler 2\n",
      "pooler 1\n",
      "pooler 2\n",
      "pooler 1\n",
      "result tensor([ 0.3301,  0.0699,  0.0878,  0.0467,  0.1129,  0.0688,  0.0937,  0.1388,\n",
      "         0.1163,  0.0921,  0.0971,  0.0682,  0.0464,  0.0215,  0.1001,  0.0965,\n",
      "         0.1035,  0.1019,  0.0449,  0.0837,  0.0670,  0.0324,  0.0671,  0.1241,\n",
      "         0.1254,  0.1053,  0.0699,  0.0986,  0.1372,  0.0624,  0.0926,  0.0297,\n",
      "         0.0942,  0.0581,  0.0105,  0.0893,  0.0601,  0.0824,  0.0371,  0.0818,\n",
      "         0.0427,  0.0666,  0.0814,  0.0809,  0.0966,  0.1066,  0.0816,  0.0798,\n",
      "         0.0457,  0.0236,  0.0937,  0.0245,  0.0963,  0.0568,  0.1128,  0.0932,\n",
      "         0.0852,  0.0112,  0.0855,  0.0723,  0.0734,  0.1152,  0.0364,  0.0669,\n",
      "         0.1294,  0.1319,  0.0656,  0.0865,  0.1503,  0.1026, -0.0015, -0.0073,\n",
      "         0.0438,  0.0311,  0.0442,  0.0780,  0.0931,  0.0810,  0.1000,  0.0424,\n",
      "         0.0703,  0.0832,  0.0718,  0.1032,  0.1104,  0.1514,  0.1361,  0.0426,\n",
      "         0.0460,  0.1147,  0.0800,  0.0712,  0.0591,  0.0650,  0.0366,  0.0811,\n",
      "         0.0819,  0.0920,  0.0475, -0.0005,  0.0501,  0.0849,  0.0446,  0.1115,\n",
      "         0.0601,  0.1314,  0.0865,  0.1120,  0.1867,  0.1744,  0.1759,  0.2289,\n",
      "         0.1554,  0.1507,  0.1842,  0.2110,  0.2060,  0.1669,  0.2743,  0.2151,\n",
      "         0.1972,  0.1646,  0.2117,  0.2453,  0.1768,  0.1894,  0.2857,  0.2400,\n",
      "         0.2228,  0.2208,  0.2011,  0.1532,  0.1630,  0.1978,  0.1753,  0.1441,\n",
      "         0.1919,  0.2079,  0.1938,  0.2590,  0.2030,  0.1700,  0.1984,  0.2083,\n",
      "         0.2023,  0.1936,  0.1813,  0.1652,  0.1870,  0.2111,  0.1985,  0.1731,\n",
      "         0.1734,  0.1525,  0.1828,  0.2055,  0.2117,  0.1507,  0.1258,  0.1686,\n",
      "         0.2293,  0.2544,  0.2150,  0.2184,  0.2166,  0.2097,  0.1212,  0.1463,\n",
      "         0.1999,  0.2423,  0.2639,  0.2068,  0.2313,  0.2358,  0.2298,  0.2249,\n",
      "         0.2377,  0.1945,  0.1941,  0.2777,  0.2828,  0.2763,  0.3134,  0.2885,\n",
      "         0.2727,  0.2144,  0.2528,  0.2563,  0.2362,  0.2428,  0.2462,  0.2324,\n",
      "         0.2385,  0.2203,  0.2146,  0.2371,  0.2277,  0.2316,  0.2476,  0.2447,\n",
      "         0.2477,  0.2544,  0.2632,  0.2643,  0.1697,  0.2528,  0.2063,  0.2299,\n",
      "         0.2335,  0.2484,  0.2367,  0.2449,  0.2498,  0.2559,  0.2457,  0.2248,\n",
      "         0.2404,  0.2342,  0.2497,  0.2363,  0.2283,  0.2468,  0.2457,  0.2323,\n",
      "         0.2483,  0.2358,  0.2369,  0.2324,  0.2368,  0.2578,  0.2362,  0.2287,\n",
      "         0.2322,  0.2211,  0.2207,  0.2392,  0.2445,  0.2300,  0.2352,  0.2205,\n",
      "         0.2119,  0.2259,  0.2151,  0.2148,  0.2003,  0.2296,  0.2324,  0.2217,\n",
      "         0.2279,  0.2451,  0.2323,  0.2260,  0.2384,  0.2372,  0.2320,  0.2418,\n",
      "         0.2303,  0.2250,  0.2391,  0.2355,  0.2504,  0.2379,  0.2482,  0.2188,\n",
      "         0.2341,  0.2377,  0.2397,  0.2388,  0.2188,  0.2252,  0.2363,  0.2472,\n",
      "         0.2549,  0.2413,  0.2337,  0.2029,  0.2101,  0.2384,  0.2394,  0.2474,\n",
      "         0.2297,  0.2366,  0.2303,  0.2234,  0.2280,  0.2391,  0.2232,  0.2402,\n",
      "         0.2238,  0.2217,  0.2347,  0.2216,  0.2543,  0.2645,  0.2524,  0.2501,\n",
      "         0.2338,  0.1958,  0.2311,  0.2499,  0.2317,  0.2340,  0.2478,  0.2157,\n",
      "         0.2377,  0.2465,  0.2268,  0.2376,  0.2303,  0.2325,  0.2264,  0.2365,\n",
      "         0.2263,  0.2396,  0.2301,  0.2304,  0.2344,  0.2466,  0.2227,  0.2362,\n",
      "         0.2315,  0.2449,  0.2299,  0.2313,  0.2623,  0.2406,  0.2406,  0.2257,\n",
      "         0.2320,  0.2204,  0.2274,  0.2278,  0.2373,  0.2609,  0.2598,  0.2516,\n",
      "         0.2707,  0.2497,  0.2224,  0.2322,  0.2361,  0.2257,  0.2241,  0.2397,\n",
      "         0.2251,  0.2139,  0.2343,  0.2239,  0.2327,  0.2221,  0.2328,  0.2256,\n",
      "         0.2519,  0.2217,  0.2322,  0.2268,  0.2416,  0.2461,  0.2391,  0.2510,\n",
      "         0.2356,  0.2211,  0.2352,  0.2305,  0.2327,  0.2468,  0.2381,  0.2543,\n",
      "         0.2388,  0.2349,  0.2244,  0.2388,  0.2273,  0.2294,  0.2781,  0.2263,\n",
      "         0.2249,  0.2375,  0.2224,  0.2159,  0.2228,  0.2198,  0.2340,  0.2261,\n",
      "         0.2460,  0.2246,  0.2268,  0.2411,  0.2199,  0.2417,  0.2376,  0.2567,\n",
      "         0.2481,  0.2324,  0.2295,  0.2399,  0.2383,  0.2380,  0.2476,  0.2506,\n",
      "         0.2384,  0.2392,  0.2389,  0.2551,  0.2120,  0.2210,  0.2319,  0.1963,\n",
      "         0.2029,  0.2216,  0.2392,  0.2282,  0.2308,  0.2250,  0.2525,  0.2306,\n",
      "         0.2329,  0.2326,  0.2329,  0.2372,  0.2554,  0.2439,  0.2534,  0.2469,\n",
      "         0.2263,  0.2254,  0.2219,  0.2311,  0.2357,  0.2386,  0.2388,  0.2282,\n",
      "         0.2338,  0.2152,  0.2449,  0.2404,  0.2525,  0.2237,  0.2501,  0.2298,\n",
      "         0.2664,  0.2467,  0.2377,  0.2573,  0.2686,  0.2599,  0.2558,  0.2427,\n",
      "         0.2658,  0.2337,  0.2464,  0.2414,  0.2410,  0.2421,  0.2266,  0.2296,\n",
      "         0.2424,  0.2571,  0.2423,  0.2307,  0.2463,  0.2374,  0.2184,  0.2351,\n",
      "         0.2376,  0.2251,  0.2372,  0.2226,  0.2235,  0.2276,  0.2272,  0.2271,\n",
      "         0.2375,  0.2358,  0.2155,  0.2404,  0.2144,  0.2434,  0.2198,  0.2311,\n",
      "         0.2464,  0.2363,  0.2396,  0.2271,  0.2300,  0.2681,  0.2431,  0.2230,\n",
      "         0.2454,  0.2033,  0.2170,  0.2476,  0.2433,  0.2371,  0.2381,  0.2321,\n",
      "         0.2425,  0.2456,  0.2424,  0.2154,  0.2310,  0.2369,  0.2371,  0.2296,\n",
      "         0.2400,  0.2393,  0.2397,  0.2524,  0.2298,  0.2299,  0.2242,  0.2594])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "answer = 'gak tau saya bu'\n",
    "ReferenceAnswer = \"5 Dampak Negatif Perkembangan Teknologi:\\nKetergantungan Berlebihan: Orang-orang menjadi terlalu bergantung pada teknologi dan kehilangan kemampuan untuk menyelesaikan masalah sendiri.\\nKecanduan Gadget: Penggunaan gadget yang berlebihan dapat menyebabkan kecanduan dan mengganggu kesehatan mental dan fisik.\\nKesenjangan Digital: Tidak semua orang memiliki akses yang sama terhadap teknologi, sehingga terjadi kesenjangan antara yang kaya dan yang miskin.\\nKejahatan Siber: Teknologi dapat digunakan untuk melakukan kejahatan, seperti penipuan, pencurian data, dan penyebaran berita bohong.\\nPenyalahgunaan Informasi: Informasi yang beredar di internet tidak selalu akurat dan dapat disalahgunakan untuk menyebarkan kebencian dan propaganda.\"\n",
    "\n",
    "# Dataset Creation (Tokenizer)\n",
    "class EssayDataTokenizer(Dataset):\n",
    "    def __init__(self, answers, students, tokenizer, max_length):\n",
    "        self.answers = answers\n",
    "        self.students = students\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        answer = self.answers[idx]\n",
    "        student = self.students[idx]\n",
    "\n",
    "        answer_encoding = self.tokenizer.encode_plus(\n",
    "            answer,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        student_encoding = self.tokenizer.encode_plus(\n",
    "            student,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'answer_text' : answer,\n",
    "            'answer_input_ids': answer_encoding['input_ids'].flatten(),\n",
    "            'answer_attention_mask': answer_encoding['attention_mask'].flatten(),\n",
    "            'student_text' : student,\n",
    "            'student_input_ids': student_encoding['input_ids'].flatten(),\n",
    "            'student_attention_mask': student_encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "class IndoBERTForSTS(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(IndoBERTForSTS, self).__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        print('pooler',len(outputs))\n",
    "        return outputs[0]\n",
    "\n",
    "# Function to calculate cosine similarity using PyTorch operations\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = nn.CosineSimilarity(dim=1)\n",
    "    return cos_sim(a, b)\n",
    "\n",
    "# function to test the model\n",
    "def test_model(model, data_loader,device):\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            print('ini d',d)\n",
    "            answer_input_ids = d[\"answer_input_ids\"].to(device)\n",
    "            answer_attention_mask = d[\"answer_attention_mask\"].to(device)\n",
    "            student_input_ids = d[\"student_input_ids\"].to(device)\n",
    "            student_attention_mask = d[\"student_attention_mask\"].to(device)\n",
    "\n",
    "            answer_outputs = model(\n",
    "                input_ids=answer_input_ids,\n",
    "                attention_mask=answer_attention_mask\n",
    "            )\n",
    "\n",
    "            student_outputs = model(\n",
    "                input_ids=student_input_ids,\n",
    "                attention_mask=student_attention_mask\n",
    "            )\n",
    "\n",
    "            cosine_scores = cosine_sim(answer_outputs, student_outputs)\n",
    "\n",
    "    return cosine_scores\n",
    "\n",
    "def runExperiment(tokenizer, bert_model):\n",
    "    # create essay dataset of test\n",
    "    test_dataset = EssayDataTokenizer(\n",
    "        [ReferenceAnswer],\n",
    "        [answer],\n",
    "        tokenizer,\n",
    "        512\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    model = IndoBERTForSTS(bert_model)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device used :',device)\n",
    "    model.to(device)\n",
    "\n",
    "    test_result = test_model(model, test_data_loader,device)\n",
    "\n",
    "    return test_result\n",
    "\n",
    "# Load IndoBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "bert_model = BertModel.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "\n",
    "# model creation and assignment\n",
    "model = IndoBERTForSTS(bert_model)\n",
    "\n",
    "result = runExperiment(tokenizer=tokenizer, bert_model=model)\n",
    "print('result', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adima\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Number  1 -----\n",
      "1 ldr light dependent resistor fungsi sensor cahaya yang mengubah intensitas cahaya menjadi nilai resistansi sensor fotodioda yang sensitif terhadap cahaya aplikasi lampu otomatis alarm pencuri sensor kamera 2 infrared inframerah fungsi mendeteksi objek berdasarkan radiasi panas yang dipancarkan sensor fotodioda atau foto transistor yang sensitif terhadap cahaya inframerah aplikasi remote control sensor suhu sensor gerakan 3 ultrasonic fungsi mengukur jarak dengan menggunakan gelombang suara ultrasonik sensor transduser yang menghasilkan dan menerima gelombang suara ultrasonik aplikasi sensor parkir sensor level air sensor robot sapi atau lembu adalah hewan ternak anggota famili bovidae dan subfamili bovinae sapi dipelihara terutama untuk dimanfaatkan susu dan dagingnya sebagai pangan manusia hasil sampingannya seperti kulit jeroan tanduk dan kotorannya juga dimanfaatkan untuk berbagai keperluan manusia sektor peternakan di kabupaten bintan dalam beberapa tahun terakhir menunjukan peningkatan populasi yang sangat menggembirakan untuk ternak sapi jumlah populasi pada tahun 2016 tercatat 725 ekor jumlah ini meningkat menjadi 978 ekor pada tahun 2019 dan menjadi 1 078 ekor pada tahun 2021 jumlah peningkatan populasi ternak sapi ini salah satunya dikarenakan kabupaten bintan melalui kementrian pertanian telah mencanangkan program upsus siwab upaya khusus sapi indukan wajib bunting sektor peternakan di kabupaten bintan walaupun bukan menjadi sektor utama dalam program pembangunan yang dilaksanakan pemerintah daerah namun merupakan faktor yang sama sekali tidak bisa dipandang sebelah mata karena sektor peternakan memberikan kontribusi yang sangat besar bagi indeks peningkatan daya beli masyarakat dan perekonomian masyarakat ternak sapi potong merupakan salah satu ternak yang banyak dibudidayakan dan diusahakan petani di kabupaten bintan ternak tersebut berperan sebagai sumber pendapatan membuka kesempatan kerja dan sumber protein hewani populasi ternak sapi potong yang tinggi menunjukkan salah satu potensi dan peluang yang dapat dimanfaatkan untuk memberikan nilai tambah dalam usaha ternak sapi meningkatkan konsumsi gizi keluarga akan protein hewani bahkan sebagai komoditas agribisnis peluang pengembangan sapi potong cukup besar hal ini dipengaruhi oleh beberapa faktor antara lain tersedianya sapi bakalan dalam jumlah besar dan mutu yang relatif baik tersedianya pakan ternak dalam jumlah cukup tersedianya lahan pangonan relatif mudahnya akses pemasaran keterampilan petani yang memadai sosial budaya yang menunjang dan adanya dukungan baik dari pihak swasta atau pemerintah\n",
      "inputs {'input_ids': tensor([[    2,   111, 30219,  ...,     0,     0,     0],\n",
      "        [    2,  4131,   158,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "rb [[0.46112305]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import torch\n",
    "from torch import clamp\n",
    "from transformers import BertTokenizer,BertModel,BertConfig\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TokenSimilarity:\n",
    "\n",
    "    def load_pretrained(self,model):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "        self.model = BertModel.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "        \n",
    "    def __cleaning(self, text:str):\n",
    "        # Replace punctuations with space\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "\n",
    "        # Clear multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Replace newlines with space\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "        return text.lower()\n",
    "        \n",
    "    def __process(self, first_token:str, second_token:str):\n",
    "        inputs = self.tokenizer([first_token, second_token],\n",
    "                                max_length=self.max_length,\n",
    "                                truncation=self.truncation,\n",
    "                                padding=self.padding,\n",
    "                                return_tensors='pt')\n",
    "\n",
    "        print('inputs',inputs)\n",
    "        attention = inputs.attention_mask\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # get the weights from the last layer as embeddings\n",
    "        embeddings = outputs.last_hidden_state# when used in older transformers version\n",
    "        # embeddings = outputs.last_hidden_state # when used in newer one\n",
    "\n",
    "        # add more dimension then expand tensor\n",
    "        # to match embeddings shape by duplicating its values by rows\n",
    "        mask = attention.unsqueeze(-1).expand(embeddings.shape).float()\n",
    "\n",
    "        masked_embeddings = embeddings * mask\n",
    "        \n",
    "        # MEAN POOLING FOR 2ND DIMENSION\n",
    "        # first, get sums by 2nd dimension\n",
    "        # second, get counts of 2nd dimension\n",
    "        # third, calculate the mean, i.e. sums/counts\n",
    "        summed = masked_embeddings.sum(1)\n",
    "        counts = clamp(mask.sum(1), min=1e-9)\n",
    "        mean_pooled = summed/counts\n",
    "\n",
    "        # return mean pooling as numpy array\n",
    "        return mean_pooled.detach().numpy()\n",
    "        \n",
    "    def predict(self, first_token:str, second_token:str,\n",
    "                return_as_embeddings:bool=False, max_length:int=512,\n",
    "                truncation:bool=True, padding:str=\"max_length\"):\n",
    "        self.max_length = max_length\n",
    "        self.truncation = truncation\n",
    "        self.padding = padding\n",
    "\n",
    "        first_token = self.__cleaning(first_token)\n",
    "        second_token = self.__cleaning(second_token)\n",
    "\n",
    "        print(first_token, second_token)\n",
    "\n",
    "        mean_pooled_arr = self.__process(first_token, second_token)\n",
    "        if return_as_embeddings:\n",
    "            return mean_pooled_arr\n",
    "\n",
    "        # calculate similarity\n",
    "        similarity = cosine_similarity([mean_pooled_arr[0]], [mean_pooled_arr[1]])\n",
    "\n",
    "        return similarity\n",
    "    \n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('savedModels/savedModel-IndoBERT-base-exEpoch20-fold5.pth',map_location=torch.device('cpu'))\n",
    "\n",
    "# Adjust the state dictionary keys\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key.replace(\"bert.\", \"\")\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "# Define the configuration for your BERT model\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p2', num_labels=2)\n",
    "\n",
    "model = BertModel(config)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "modelPred = TokenSimilarity()\n",
    "modelPred.load_pretrained(model)\n",
    "\n",
    "token1 = \"\"\"1. LDR (Light Dependent Resistor): Fungsi: Sensor cahaya yang mengubah intensitas cahaya menjadi nilai resistansi. Sensor: Fotodioda yang sensitif terhadap cahaya. Aplikasi: Lampu otomatis, alarm pencuri, sensor kamera. 2. Infrared (Inframerah): Fungsi: Mendeteksi objek berdasarkan radiasi panas yang dipancarkan. Sensor: Fotodioda atau foto transistor yang sensitif terhadap cahaya inframerah. Aplikasi: Remote control, sensor suhu, sensor gerakan. 3. Ultrasonic: Fungsi: Mengukur jarak dengan menggunakan gelombang suara ultrasonik. Sensor: Transduser yang menghasilkan dan menerima gelombang suara ultrasonik. Aplikasi: Sensor parkir, sensor level air, sensor robot.\"\"\"\n",
    "\n",
    "# token2 = \"\"\"\n",
    "# Storage Device : Hardware yang dapat menyimpan informasi dan data, baik itu dalam perangkat komputer maupun di luar\\nInput Device : Perangkat keras komputer yang berfungsi untuk melakukan pemasukan data\\nOutput Device : yang berfungsi untuk melakukan pengeluaran data\n",
    "# \"\"\"\n",
    "\n",
    "token2 = \"\"\"\n",
    "Sapi atau Lembu adalah hewan ternak anggota famili Bovidae dan subfamili Bovinae. Sapi dipelihara terutama untuk dimanfaatkan susu dan dagingnya sebagai pangan manusia. Hasil sampingannya seperti kulit, jeroan, tanduk, dan kotorannya juga dimanfaatkan untuk berbagai keperluan manusia. Sektor Peternakan di kabupaten Bintan dalam beberapa tahun terakhir menunjukan peningkatan populasi yang sangat menggembirakan, untuk ternak sapi, jumlah populasi pada tahun 2016 tercatat 725 ekor, jumlah ini meningkat menjadi 978 ekor pada tahun 2019 dan menjadi 1.078 ekor pada tahun 2021. Jumlah peningkatan populasi ternak sapi ini, salah satunya dikarenakan Kabupaten Bintan melalui Kementrian Pertanian telah mencanangkan Program Upsus Siwab (Upaya Khusus Sapi indukan Wajib Bunting). Sektor peternakan di Kabupaten Bintan walaupun bukan menjadi sektor utama dalam program pembangunan yang dilaksanakan pemerintah daerah, namun merupakan faktor yang sama sekali tidak bisa dipandang sebelah mata karena sektor peternakan memberikan kontribusi yang sangat besar bagi indeks peningkatan daya beli masyarakat dan perekonomian masyarakat. Ternak sapi potong merupakan salah satu ternak yang banyak dibudidayakan dan diusahakan petani di Kabupaten Bintan. Ternak tersebut berperan sebagai sumber pendapatan, membuka kesempatan kerja dan sumber protein hewani. Populasi ternak sapi potong yang tinggi menunjukkan salah satu potensi dan peluang yang dapat dimanfaatkan untuk memberikan nilai tambah dalam usaha ternak sapi, meningkatkan konsumsi gizi keluarga akan protein hewani bahkan sebagai komoditas agribisnis. Peluang pengembangan sapi potong cukup besar, hal ini dipengaruhi oleh beberapa faktor antara lain tersedianya sapi bakalan dalam jumlah besar dan mutu yang relatif baik, tersedianya pakan ternak dalam jumlah cukup, tersedianya lahan pangonan, relatif mudahnya akses pemasaran, keterampilan petani yang memadai, sosial budaya yang menunjang dan adanya dukungan baik dari pihak swasta atau pemerintah.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokenArr = [token2]\n",
    "\n",
    "for idx in range(len(tokenArr)):\n",
    "  print('----Number ',idx+1,'-----')\n",
    "  resultBERT = modelPred.predict(token1,tokenArr[idx])\n",
    "  print('rb',resultBERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
